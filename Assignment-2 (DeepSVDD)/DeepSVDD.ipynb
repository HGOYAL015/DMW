{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "DeepSVDD.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeMmPMXyu0tT",
        "outputId": "902db453-f026-4fc7-f8c6-f976aff69067"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "!pip install train\n",
        "!pip install barbar\n",
        "\n",
        "from train import TrainerDeepSVDD\n",
        "from preprocess import get_mnist\n",
        "\n",
        "!pip install future\n",
        "!pip install goslate\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: train in /usr/local/lib/python3.7/dist-packages (0.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from train) (1.19.5)\n",
            "Requirement already satisfied: barbar in /usr/local/lib/python3.7/dist-packages (0.2.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (0.16.0)\n",
            "Requirement already satisfied: goslate in /usr/local/lib/python3.7/dist-packages (1.5.1)\n",
            "Requirement already satisfied: futures in /usr/local/lib/python3.7/dist-packages (from goslate) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijA3reawu0tW",
        "outputId": "3182393d-a538-4e2d-e2b3-33c3e8e91696"
      },
      "source": [
        "class argum:\n",
        "    batch_size=200\n",
        "    pretrain=True\n",
        "    num_epochs=150\n",
        "    num_epochs_ae=150\n",
        "    latent_dim=32\n",
        "    patience=50\n",
        "    lr=1e-4\n",
        "    weight_decay=0.5e-6\n",
        "    weight_decay_ae=0.5e-3\n",
        "    lr_ae=1e-4\n",
        "    lr_milestones=[50]\n",
        "    normal_class=1\n",
        "    \n",
        "    \n",
        "A=argum()\n",
        "dvc=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "dt=get_mnist(A)\n",
        "dpSVDD=TrainerDeepSVDD(A, dt, dvc)\n",
        "\n",
        "if A.pretrain:\n",
        "    dpSVDD.pretrain()\n",
        "\n",
        "dpSVDD.train()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 0, Loss: 170.579\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 1, Loss: 127.885\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 2, Loss: 93.969\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 3, Loss: 68.828\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 4, Loss: 51.361\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 5, Loss: 39.498\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 6, Loss: 31.360\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 7, Loss: 25.653\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 8, Loss: 21.500\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 9, Loss: 18.400\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 10, Loss: 16.015\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 11, Loss: 14.139\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 12, Loss: 12.631\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 13, Loss: 11.396\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 14, Loss: 10.375\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 15, Loss: 9.511\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 16, Loss: 8.777\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 17, Loss: 8.145\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 18, Loss: 7.589\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 19, Loss: 7.097\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 20, Loss: 6.658\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 21, Loss: 6.266\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 22, Loss: 5.913\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 23, Loss: 5.596\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 24, Loss: 5.312\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 25, Loss: 5.053\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 26, Loss: 4.817\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 27, Loss: 4.591\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 28, Loss: 4.384\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 29, Loss: 4.191\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 30, Loss: 4.016\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 31, Loss: 3.855\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 32, Loss: 3.710\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 33, Loss: 3.571\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 34, Loss: 3.446\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 35, Loss: 3.330\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 36, Loss: 3.225\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 37, Loss: 3.124\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 38, Loss: 3.032\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 39, Loss: 2.945\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 40, Loss: 2.863\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 41, Loss: 2.786\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 42, Loss: 2.713\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 43, Loss: 2.646\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 44, Loss: 2.580\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 45, Loss: 2.517\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 46, Loss: 2.458\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 47, Loss: 2.403\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 48, Loss: 2.348\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 49, Loss: 2.296\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 50, Loss: 2.265\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 51, Loss: 2.261\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 52, Loss: 2.255\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 53, Loss: 2.250\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 54, Loss: 2.245\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 55, Loss: 2.239\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 56, Loss: 2.234\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 57, Loss: 2.228\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 58, Loss: 2.221\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 59, Loss: 2.216\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 60, Loss: 2.211\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 61, Loss: 2.204\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 62, Loss: 2.199\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 63, Loss: 2.193\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 64, Loss: 2.188\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 65, Loss: 2.181\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 66, Loss: 2.175\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 67, Loss: 2.170\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 68, Loss: 2.164\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 69, Loss: 2.158\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 70, Loss: 2.153\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 71, Loss: 2.145\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 72, Loss: 2.140\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 73, Loss: 2.134\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 74, Loss: 2.126\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 75, Loss: 2.120\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 76, Loss: 2.114\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 77, Loss: 2.106\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 78, Loss: 2.100\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 79, Loss: 2.093\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 80, Loss: 2.087\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 81, Loss: 2.079\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 82, Loss: 2.073\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 83, Loss: 2.066\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 84, Loss: 2.060\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 85, Loss: 2.052\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 86, Loss: 2.045\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 87, Loss: 2.038\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 88, Loss: 2.030\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 89, Loss: 2.023\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 90, Loss: 2.016\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 91, Loss: 2.008\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 92, Loss: 1.999\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 93, Loss: 1.992\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 94, Loss: 1.984\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 95, Loss: 1.976\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 96, Loss: 1.969\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 97, Loss: 1.961\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 98, Loss: 1.954\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 99, Loss: 1.944\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 100, Loss: 1.936\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 101, Loss: 1.927\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 102, Loss: 1.918\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 103, Loss: 1.911\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 104, Loss: 1.901\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 105, Loss: 1.893\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 106, Loss: 1.885\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 107, Loss: 1.876\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 108, Loss: 1.868\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 109, Loss: 1.859\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 110, Loss: 1.850\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 111, Loss: 1.841\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 112, Loss: 1.832\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 113, Loss: 1.824\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 114, Loss: 1.815\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 115, Loss: 1.807\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 116, Loss: 1.800\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 117, Loss: 1.790\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 118, Loss: 1.781\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 119, Loss: 1.771\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 120, Loss: 1.765\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 121, Loss: 1.756\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 122, Loss: 1.748\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 123, Loss: 1.741\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 124, Loss: 1.731\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 125, Loss: 1.723\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 126, Loss: 1.715\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 127, Loss: 1.707\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 128, Loss: 1.699\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 129, Loss: 1.690\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 130, Loss: 1.683\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 131, Loss: 1.675\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 132, Loss: 1.665\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 133, Loss: 1.657\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 134, Loss: 1.649\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 135, Loss: 1.640\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 136, Loss: 1.633\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 137, Loss: 1.624\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 138, Loss: 1.616\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 139, Loss: 1.609\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 140, Loss: 1.599\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 141, Loss: 1.591\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 142, Loss: 1.583\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 143, Loss: 1.575\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 144, Loss: 1.568\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 145, Loss: 1.560\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 146, Loss: 1.551\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 147, Loss: 1.545\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 148, Loss: 1.537\n",
            "6742/6742: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 149, Loss: 1.529\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 0, Loss: 0.837\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 1, Loss: 0.259\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 2, Loss: 0.086\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 3, Loss: 0.049\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 4, Loss: 0.036\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 5, Loss: 0.029\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Training Deep SVDD... Epoch: 6, Loss: 0.024\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 7, Loss: 0.020\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 8, Loss: 0.018\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 9, Loss: 0.016\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 10, Loss: 0.014\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 11, Loss: 0.013\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 12, Loss: 0.012\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 13, Loss: 0.011\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 14, Loss: 0.010\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 15, Loss: 0.010\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 16, Loss: 0.009\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 17, Loss: 0.009\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 18, Loss: 0.008\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 19, Loss: 0.008\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 20, Loss: 0.007\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 21, Loss: 0.007\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 22, Loss: 0.007\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 23, Loss: 0.006\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 24, Loss: 0.006\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 25, Loss: 0.006\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 26, Loss: 0.006\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 27, Loss: 0.006\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 28, Loss: 0.005\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 29, Loss: 0.005\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 30, Loss: 0.005\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 31, Loss: 0.005\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 32, Loss: 0.005\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 33, Loss: 0.005\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 34, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 35, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 36, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 37, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 38, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 39, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 40, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 41, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 42, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 43, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 44, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 45, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 46, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 47, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 48, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 49, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 50, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 51, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 52, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 53, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 54, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 55, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 56, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 57, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 58, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 59, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 60, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 61, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 62, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 63, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 64, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 65, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 66, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 67, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 68, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 69, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 70, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 71, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 72, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 73, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 74, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 75, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 76, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 77, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 78, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 79, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 80, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 81, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 82, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 83, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 84, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 85, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 86, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 87, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 88, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 89, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 90, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Training Deep SVDD... Epoch: 91, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 92, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 93, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 94, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Training Deep SVDD... Epoch: 95, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 96, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 97, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 98, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 99, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 100, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 101, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 102, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 103, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 104, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Training Deep SVDD... Epoch: 105, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 106, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 107, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 108, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 109, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 110, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 111, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 112, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 113, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 114, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 115, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 116, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 117, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 118, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 119, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Training Deep SVDD... Epoch: 120, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 121, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 122, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 123, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 124, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 125, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 126, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 127, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 128, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 129, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Training Deep SVDD... Epoch: 130, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 131, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 132, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 133, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 134, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Training Deep SVDD... Epoch: 135, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 136, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 137, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 138, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 139, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 140, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 141, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 142, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 143, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 144, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 145, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 146, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 147, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 148, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 149, Loss: 0.002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KriHZ7eu0tY",
        "outputId": "1351074b-2ba1-4617-b13a-81d776c0f043"
      },
      "source": [
        "from test import eval\n",
        "lb,sc=eval(dpSVDD.net,dpSVDD.c,dt[1],dvc)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing...\n",
            "ROC AUC score: 99.54\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "6cELeVonu0tY",
        "outputId": "93e26510-9ac6-42ab-e294-d25fafde6c85"
      },
      "source": [
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "scIn=sc[np.where(lb==0)[0]]\n",
        "scOut=sc[np.where(lb==1)[0]]\n",
        "\n",
        "inn=pd.DataFrame(scIn,columns=['Inlier'])\n",
        "outt=pd.DataFrame(scOut,columns=['Outlier'])\n",
        "\n",
        "\n",
        "fig,ax=plt.subplots()\n",
        "inn.plot.kde(ax=ax,legend=True,title='Outliers vs Inliers (Deep SVDD)')\n",
        "outt.plot.kde(ax=ax, legend=True)\n",
        "plt.xlim(-0.05, 0.08)\n",
        "ax.grid(axis='x')\n",
        "ax.grid(axis='y')\n",
        "plt.show()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}